<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Multimodel: Speech-Text Representation Learning | aklywtx</title>
<meta name=keywords content="Multimodal"><meta name=description content="About Speech Processing
Tasks
There are many speech processing tasks, so the key to a good speech+LLM is to generalize well!

automatic speech recognition (ASR)
speech question answering (SQA)
speech-to-text translation (ST)
text-to-speech(TTS)
emotion recognition (ER)
speaker verification (SV)
automatic speech translation(AST): translating the audio to obtain the translated transcript.
S2ST (speech-to-speech translation): translating the audio to obtain the translated audio.

Datasets
LibriSpeech: English reading speech
AMI: multi-talker meeting recordings"><meta name=author content="Me"><link rel=canonical href=https://aklywtx.github.io/posts/speech-text-representation-learning/><link crossorigin=anonymous href=/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as=style><link rel=icon href=https://aklywtx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://aklywtx.github.io/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://aklywtx.github.io/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://aklywtx.github.io/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://aklywtx.github.io/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://aklywtx.github.io/posts/speech-text-representation-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://aklywtx.github.io/posts/speech-text-representation-learning/"><meta property="og:site_name" content="aklywtx"><meta property="og:title" content="Multimodel: Speech-Text Representation Learning"><meta property="og:description" content="About Speech Processing Tasks There are many speech processing tasks, so the key to a good speech+LLM is to generalize well!
automatic speech recognition (ASR) speech question answering (SQA) speech-to-text translation (ST) text-to-speech(TTS) emotion recognition (ER) speaker verification (SV) automatic speech translation(AST): translating the audio to obtain the translated transcript. S2ST (speech-to-speech translation): translating the audio to obtain the translated audio. Datasets LibriSpeech: English reading speech
AMI: multi-talker meeting recordings"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-01-29T00:00:00+00:00"><meta property="article:modified_time" content="2025-01-29T00:00:00+00:00"><meta property="article:tag" content="Multimodal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Multimodel: Speech-Text Representation Learning"><meta name=twitter:description content="About Speech Processing
Tasks
There are many speech processing tasks, so the key to a good speech+LLM is to generalize well!

automatic speech recognition (ASR)
speech question answering (SQA)
speech-to-text translation (ST)
text-to-speech(TTS)
emotion recognition (ER)
speaker verification (SV)
automatic speech translation(AST): translating the audio to obtain the translated transcript.
S2ST (speech-to-speech translation): translating the audio to obtain the translated audio.

Datasets
LibriSpeech: English reading speech
AMI: multi-talker meeting recordings"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://aklywtx.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Multimodel: Speech-Text Representation Learning","item":"https://aklywtx.github.io/posts/speech-text-representation-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Multimodel: Speech-Text Representation Learning","name":"Multimodel: Speech-Text Representation Learning","description":"About Speech Processing Tasks There are many speech processing tasks, so the key to a good speech+LLM is to generalize well!\nautomatic speech recognition (ASR) speech question answering (SQA) speech-to-text translation (ST) text-to-speech(TTS) emotion recognition (ER) speaker verification (SV) automatic speech translation(AST): translating the audio to obtain the translated transcript. S2ST (speech-to-speech translation): translating the audio to obtain the translated audio. Datasets LibriSpeech: English reading speech\nAMI: multi-talker meeting recordings\n","keywords":["Multimodal"],"articleBody":"About Speech Processing Tasks There are many speech processing tasks, so the key to a good speech+LLM is to generalize well!\nautomatic speech recognition (ASR) speech question answering (SQA) speech-to-text translation (ST) text-to-speech(TTS) emotion recognition (ER) speaker verification (SV) automatic speech translation(AST): translating the audio to obtain the translated transcript. S2ST (speech-to-speech translation): translating the audio to obtain the translated audio. Datasets LibriSpeech: English reading speech\nAMI: multi-talker meeting recordings\nFisher: 2-channel English telephony conversations\nSwitchboard: 2-channel English telephony conversations\n‚Ä¶..\nCurrent SOTA speech encoders Whisper: Whisper is trained for ASR and ST tasks in a weakly supervised fashion on a massive 680khour speech corpus recorded in diverse conditions. WavLM: WavLM is a predictive based self-supervised learning (SSL) pre-trained model. During its pre-training stage, WavLM mixes each utterance with signals from multiple speakers in the same batch, yet selectively predicts only the targets associated with the utterance‚Äôs original speaker. (Speech and audio are different)\nPaper-1: SALMONN Titel: SALMONN: TOWARDS GENERIC HEARING ABILITIES FOR LARGE LANGUAGE MODELS\nSALMONN: Speech Audio Language Music Open Neural Network\nKey Idea SALMONN: Integrating **a pre-trained text-based LLM with speech and audio encoders into a single multimodal model which can perceive and understand 3 types of audio(speech, audio events and music).\nContribution achieve competitive performances on speech related tasks (automatic speech recognition and translation, auditoryinformation-based question answering, emotion recognition, speaker verification, music and audio captioning) emergent abilities unseen in the training (speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning) The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities Model architecture Dual Audito Encoders‚ùÑÔ∏è Whisper - speech BEATs - non-speech Complement each other Q-Formerüî• Convert Audio Encoder output to Text Tokens; Connecting Component between Audio Encoders and LLM Window-level, better for varaible length has trainable queries (N=1) LLM (Vicuna)‚ùÑÔ∏è + LoRAüî• Use instruction prompt to finetuning LLM Training stages Stage 1: Pre-training\nTo Learn high-quality alignment between auditory and textual inputs. Tasks: Speech Recognition; Audio Captioning (Non-Speech) Stage 2: Instruction Tuning\nTask format: Audio + Text + Prompts Tasks: Stage1 tasks + 12 extra Tasks Stage 3: Active Instruction Tuning\nTo solve the Task Overfitting Task Overfitting: Can‚Äôt perform untrained cross-model tasks; violates prompts and generates irrelevant responses related to a task commonly seen in training like speech recognition\nPaper-2: WavLLM WavLLM: Towards Robust and Adaptive Speech Large Language Model\nKey Idea Introduce WavLLM, a speech LLM aiming at enhancing the generalization capabilies. WavLLM features dual encoders‚Äîa Whisper encoder for semantics and a WavLM encoder for speaker characteristics.\nResearch Gap: Ineffective eneralization in previous research specialized tasks are highly sensitive to prompt design, resulting in performance degradation when confronted with unseen or complex instructions An absence of speech Chain-of-Thought (CoT) capability, which is essential for addressing complex tasks. Curriculum learning:\nCurriculum learning is a training strategy in machine learning and artificial intelligence inspired by the way humans learn. The idea is to train models by starting with simpler tasks or data and gradually increasing the difficulty or complexity of the training examples. This approach mirrors how humans often learn better by building on fundamental skills before tackling more challenging problems.\nApplications of Curriculum Learning\nNatural Language Processing (NLP): In language models, training on simpler texts first (e.g., short or syntactically simple sentences) and progressing to more complex ones. Computer Vision: Learning to recognize simple patterns or objects before moving to intricate or cluttered images. Reinforcement Learning: Gradually introducing more complex tasks or environments for agents to learn more effectively. Speech Recognition: Starting with clear and distinct audio samples and then training on noisier or more complex data. Implementation of Curriculum Learning\nCurriculum learning can be implemented in two main ways:\nExplicit Curriculum: The designer explicitly defines the order of training data or tasks. Implicit Curriculum: The curriculum emerges naturally through algorithms that prioritize simpler examples, such as importance sampling or active learning techniques. Reducing the LoRA scaling factor can be beneficial for multi-task instructions, but leads to a substantial degradation of the results of training tasks (Tang et al., 2024), which suggests that single and multiple tasks might benefit from distinct LoRA scaling factors\nModel Architecture Summary: speech encoders(Whisper + WavLM) + modality adapters + a LLM(LLaMA-2-7B-chat) + a proposed prompt adapter\n![Screenshot 2025-01-20 at 15.19.50](/Users/aklywtx/Library/Application Support/typora-user-images/Screenshot 2025-01-20 at 15.19.50.png)\nspeech encoders: Whisper-large-v2 for semantic representations. WavLM-base for speaker-related acoustic representations, which are computed as a weighted sum of its hidden states (with learnable weights). modality adapters: Each encoder‚Äôs output is passed to its corresponding modality adapter: Semantic Adapter for Whisper; Acoustic Adapter for WavLM. Each modality adapter has: Two 1-D convolution layers for downsampling and aligning the output of both encoders within the temporal domain Down-up bottleneck adapter: Efficiently transforms the representations. Final linear projector The outputs of both adapters are concatenated along the feature dimension and linearly transformed before being fed into the LLM Prompt Adapter To enable adaptive LoRA scaling factors for different single-task and multiple-task instructions We propose an online adaptation strategy by introducing a down-up prompt-aware LoRA weight adapter (aka. prompt adapter) with attention weights, designed to modulate the effect of LoRA on LLaMA, Curriculum Learning Mixed Single-Task Training Stage Datasets: single-task, cross-modal, speech-text pairing datasets or text-only datasets optimize the modality adapters, a linear layer and LoRA components. Advanced Multi-Task Training Stage A more complex prompt-aware multi-task dataset: Multi-task and single-task datasets are utilized together Simply incorporating more challenging training data may slightly diminish the performance of single-task instructions -\u003e Prompt Adapter Paper-3: AudioPaLM Titel: AudioPaLM: A Large Language Model That Can Speak and Listen\nA unified multimodal architecture AudioPaLM\nAudioLM: paralinguistic information such as speaker identity and intonation\nPaLM-2 (LLM): linguistic knowledge\nContributions:\ninitializing AudioPaLM with the weights of a text-only large language model improves speech processing The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform zero-shot speech-to-text translation for many languages for which input/target language combinations were not seen in training. AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt. Key idea A joint vocabulary that can represent speech and text with a limited number of discrete tokens which, combined with an elementary markup description of tasks, allows training a single decoder-only model on a mixture of tasks that involve arbitrarily interleaved speech and text.\nthe underlying architecture of AudioPaLM is a large Transformer model -\u003e can use the weights of a LLM as initial weights.\nRelated work Ways to fuse text-decoders with advances in non-text encoder models:\nadapter layers: Flamingo, Whisper a separate encoder: PaLI pro of these two ways: individual components can be frozen while finetuning the model on multimodal data\ncon: such models are constrained to only output text, since the decoder is text-only\nModel Architecture ![Screenshot 2025-01-27 at 14.59.13](/Users/aklywtx/Library/Application Support/typora-user-images/Screenshot 2025-01-27 at 14.59.13.png)\nhow text and audio inputs are tokenized;\nExtract embeddings from the w2v-BERT model and quantize them via k-means. No Normalization before performing the k-means clustering because they found in the multilingual setting that normalization did cause degradation.\nAlso tried this on Universal Speech Model (USM) encoder v1, USM-v2(with an auxiliary ASR loss)\nhow we modify existing pretrained text decoders to also model audio;\nIn the token embedding matrix $E$, a $t\\times m$ matrix, they keep the first t tokens (from zero to t) which correspond to the SentencePiece text tokens while adding the next a tokens (from t to t + a) represent audio tokens. Then train all model parameters.\nhow we convert the model output into raw audio.\n‚Äã\tThey experimented with two different methods: i) autoregressive decoding, following the setup of AudioLM; ii) nonautoregressive decoding, using the recently proposed SoundStorm model\nTasks Expressing tasks\nCombined tasks (combine multiple simple tasks): instruct the model to also output intermediate steps; similar in spirit to chain of thought prompting\nPaper-4: SpeechGPT Titel: SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities\nResearch Gaps Images and speech are continuous signals. They can‚Äôt be adapted directly to LLMs which receive discrete tokens. Current speech model: cascading paradigm (the LLM is connected with an automatic speech recognition (ASR) model or a text-tospeech (TTS) model in tandem, or the LLM is employed as a control hub, with several speech processing models) -\u003e limitations: LLM‚Äôs knowledge cannot be transferred to the speech modality; loss of paralinguistic signals; spoken language models only synthesize speech but fail to comprehend its semantic information Key idea: speech discretization speech discretization with a self-supervised trained speech model to unify the modality between speech and text. The discrete speech tokens are then expanded into the vocabulary of the LLM -\u003e model can perceive and generate the speech.\ncapacity of follwing multi-modal instructions: build the first speech-text cross-modal instruction-following dataset SpeechInstruct.\nDataset: SpeechInstruct a speech-text crossmodal instruction-following dataset\nCrossModal Instruction Chain-of-Modality Instruction ","wordCount":"1479","inLanguage":"en","datePublished":"2025-01-29T00:00:00Z","dateModified":"2025-01-29T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://aklywtx.github.io/posts/speech-text-representation-learning/"},"publisher":{"@type":"Organization","name":"aklywtx","logo":{"@type":"ImageObject","url":"https://aklywtx.github.io/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://aklywtx.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://aklywtx.github.io/seal.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://aklywtx.github.io/categories/ title=categories><span>categories</span></a></li><li><a href=https://aklywtx.github.io/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://aklywtx.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://aklywtx.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Multimodel: Speech-Text Representation Learning</h1><div class=post-meta><span title='2025-01-29 00:00:00 +0000 UTC'>January 29, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;1479 words&nbsp;¬∑&nbsp;Me</div></header><div class=post-content><h2 id=about-speech-processing>About Speech Processing<a hidden class=anchor aria-hidden=true href=#about-speech-processing>#</a></h2><h3 id=tasks>Tasks<a hidden class=anchor aria-hidden=true href=#tasks>#</a></h3><p>There are many speech processing tasks, so the key to a good speech+LLM is to generalize well!</p><ul><li>automatic speech recognition (ASR)</li><li>speech question answering (SQA)</li><li>speech-to-text translation (ST)</li><li>text-to-speech(TTS)</li><li>emotion recognition (ER)</li><li>speaker verification (SV)</li><li>automatic speech translation(AST): translating the audio to obtain the translated transcript.</li><li>S2ST (speech-to-speech translation): translating the audio to obtain the translated audio.</li></ul><h3 id=datasets>Datasets<a hidden class=anchor aria-hidden=true href=#datasets>#</a></h3><p>LibriSpeech: English reading speech</p><p>AMI: multi-talker meeting recordings</p><p>Fisher: 2-channel English telephony conversations</p><p>Switchboard: 2-channel English telephony conversations</p><p>&mldr;..</p><h3 id=current-sota-speech-encoders>Current SOTA speech encoders<a hidden class=anchor aria-hidden=true href=#current-sota-speech-encoders>#</a></h3><ul><li><strong>Whisper:</strong> Whisper is trained for ASR and ST tasks in a weakly supervised fashion on a massive 680khour speech corpus recorded in diverse conditions.</li><li><strong>WavLM:</strong> WavLM is a predictive based self-supervised learning (SSL) pre-trained model. During its pre-training stage, WavLM mixes each utterance with signals from multiple speakers in the same batch, yet selectively predicts only the targets associated with the utterance‚Äôs original speaker.</li></ul><p>(Speech and audio are different)</p><h2 id=paper-1-salmonn>Paper-1: SALMONN<a hidden class=anchor aria-hidden=true href=#paper-1-salmonn>#</a></h2><p><strong>Titel: SALMONN: TOWARDS GENERIC HEARING ABILITIES FOR LARGE LANGUAGE MODELS</strong></p><p>SALMONN: Speech Audio Language Music Open Neural Network</p><h3 id=key-idea>Key Idea<a hidden class=anchor aria-hidden=true href=#key-idea>#</a></h3><p>SALMONN: Integrating **a pre-trained text-based LLM with <strong>speech and audio encoders</strong> into a single multimodal model which can perceive and understand 3 types of audio(speech, audio events and music).</p><h3 id=contribution>Contribution<a hidden class=anchor aria-hidden=true href=#contribution>#</a></h3><ul><li>achieve competitive performances on speech related tasks (automatic speech recognition and translation, auditoryinformation-based question answering, emotion recognition, speaker verification, music and audio captioning)</li><li><strong>emergent abilities</strong> unseen in the training (speech translation to untrained languages, speech-based slot filling, spoken-query-based question answering, audio-based storytelling, and speech audio co-reasoning)</li><li>The presence of cross-modal emergent abilities is studied, and a novel few-shot activation tuning approach is proposed to activate such abilities</li></ul><h3 id=model-architecture>Model architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h3><ol><li>Dual Audito Encoders‚ùÑÔ∏è<ul><li>Whisper - speech</li><li>BEATs - non-speech</li><li>Complement each other</li></ul></li><li>Q-Formerüî•<ul><li>Convert Audio Encoder output to Text Tokens; Connecting Component between Audio Encoders and LLM</li><li>Window-level, better for varaible length</li><li>has trainable queries (N=1)</li></ul></li><li>LLM (Vicuna)‚ùÑÔ∏è + LoRAüî•<ul><li>Use instruction prompt to finetuning LLM</li></ul></li></ol><h3 id=training-stages>Training stages<a hidden class=anchor aria-hidden=true href=#training-stages>#</a></h3><ul><li><p><strong>Stage 1: Pre-training</strong></p><ul><li>To Learn high-quality <strong>alignment</strong> between <em>auditory</em> and <em>textual</em> inputs.</li><li>Tasks: Speech Recognition; Audio Captioning (Non-Speech)</li></ul></li><li><p><strong>Stage 2: Instruction Tuning</strong></p><ul><li>Task format: Audio + Text + Prompts</li><li>Tasks: Stage1 tasks + 12 extra Tasks</li></ul></li><li><p><strong>Stage 3: Active Instruction Tuning</strong></p><ul><li>To solve the <em>Task Overfitting</em></li></ul><blockquote><p>Task Overfitting: Can&rsquo;t perform untrained cross-model tasks; violates prompts and generates irrelevant responses related to a task commonly seen in training like speech recognition</p></blockquote></li></ul><h2 id=paper-2-wavllm>Paper-2: WavLLM<a hidden class=anchor aria-hidden=true href=#paper-2-wavllm>#</a></h2><p><strong>WavLLM: Towards Robust and Adaptive Speech Large Language Model</strong></p><h3 id=key-idea-1>Key Idea<a hidden class=anchor aria-hidden=true href=#key-idea-1>#</a></h3><p>Introduce WavLLM, a speech LLM aiming at enhancing the <strong>generalization</strong> capabilies. WavLLM features dual encoders‚Äîa Whisper encoder for semantics and a WavLM encoder for speaker characteristics.</p><h3 id=research-gap-ineffective-eneralization-in-previous-research>Research Gap: Ineffective eneralization in previous research<a hidden class=anchor aria-hidden=true href=#research-gap-ineffective-eneralization-in-previous-research>#</a></h3><ul><li>specialized tasks are highly sensitive to prompt design, resulting in performance degradation when confronted with unseen or complex instructions</li><li>An absence of speech Chain-of-Thought (CoT) capability, which is essential for addressing complex tasks.</li></ul><blockquote><p>Curriculum learning:</p><p><strong>Curriculum learning</strong> is a training strategy in machine learning and artificial intelligence inspired by the way humans learn. The idea is to train models by starting with simpler tasks or data and gradually increasing the difficulty or complexity of the training examples. This approach mirrors how humans often learn better by building on fundamental skills before tackling more challenging problems.</p><p><strong>Applications of Curriculum Learning</strong></p><ul><li><strong>Natural Language Processing (NLP)</strong>: In language models, training on simpler texts first (e.g., short or syntactically simple sentences) and progressing to more complex ones.</li><li><strong>Computer Vision</strong>: Learning to recognize simple patterns or objects before moving to intricate or cluttered images.</li><li><strong>Reinforcement Learning</strong>: Gradually introducing more complex tasks or environments for agents to learn more effectively.</li><li><strong>Speech Recognition</strong>: Starting with clear and distinct audio samples and then training on noisier or more complex data.</li></ul><p><strong>Implementation of Curriculum Learning</strong></p><p>Curriculum learning can be implemented in two main ways:</p><ol><li><strong>Explicit Curriculum</strong>: The designer explicitly defines the order of training data or tasks.</li><li><strong>Implicit Curriculum</strong>: The curriculum emerges naturally through algorithms that prioritize simpler examples, such as importance sampling or active learning techniques.</li></ol></blockquote><blockquote><p>Reducing the LoRA scaling factor can be beneficial for multi-task instructions, but leads to a substantial degradation of the results of training tasks (Tang et al., 2024), which suggests that single and multiple tasks might benefit from distinct LoRA scaling factors</p></blockquote><h3 id=model-architecture-1>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture-1>#</a></h3><p>Summary: speech encoders(Whisper + WavLM) + modality adapters + a LLM(LLaMA-2-7B-chat) + a proposed prompt adapter</p><p>![Screenshot 2025-01-20 at 15.19.50](/Users/aklywtx/Library/Application Support/typora-user-images/Screenshot 2025-01-20 at 15.19.50.png)</p><ul><li>speech encoders:<ul><li>Whisper-large-v2 for semantic representations.</li><li>WavLM-base for speaker-related acoustic representations, which are computed as a <strong>weighted sum</strong> of its hidden states (with learnable weights).</li></ul></li><li>modality adapters:<ul><li>Each encoder‚Äôs output is passed to its corresponding <strong>modality adapter</strong>: <strong>Semantic Adapter</strong> for Whisper; <strong>Acoustic Adapter</strong> for WavLM.</li><li>Each modality adapter has:<ul><li>Two 1-D convolution layers for downsampling and aligning the output of both encoders within the temporal domain</li><li>Down-up bottleneck adapter: Efficiently transforms the representations.</li><li>Final linear projector</li></ul></li></ul></li><li>The outputs of both adapters are concatenated along the feature dimension and linearly transformed before being fed into the LLM</li><li><strong>Prompt Adapter</strong><ul><li>To enable adaptive LoRA scaling factors for different single-task and multiple-task instructions</li><li>We propose an online adaptation strategy by introducing a down-up prompt-aware LoRA weight adapter (aka. prompt adapter) with attention weights, designed to modulate the effect of LoRA on LLaMA,</li></ul></li></ul><h3 id=curriculum-learning>Curriculum Learning<a hidden class=anchor aria-hidden=true href=#curriculum-learning>#</a></h3><ol><li>Mixed Single-Task Training Stage<ul><li>Datasets: single-task, cross-modal, speech-text pairing datasets or text-only datasets</li><li>optimize the modality adapters, a linear layer and LoRA components.</li></ul></li><li>Advanced Multi-Task Training Stage<ul><li>A more complex prompt-aware multi-task dataset: Multi-task and single-task datasets are utilized together</li><li><strong>Simply incorporating more challenging training data may slightly diminish the performance of single-task instructions</strong> -> <strong>Prompt Adapter</strong></li></ul></li></ol><h2 id=paper-3-audiopalm>Paper-3: AudioPaLM<a hidden class=anchor aria-hidden=true href=#paper-3-audiopalm>#</a></h2><p><strong>Titel: AudioPaLM: A Large Language Model That Can Speak and Listen</strong></p><p>A unified multimodal architecture AudioPaLM</p><p>AudioLM: paralinguistic information such as speaker identity and intonation</p><p>PaLM-2 (LLM): linguistic knowledge</p><p>Contributions:</p><ol><li>initializing AudioPaLM with the weights of a text-only large language model improves speech processing</li><li>The resulting model significantly outperforms existing systems for speech translation tasks and has the ability to perform <strong>zero-shot speech-to-text translation</strong> for many languages for which <strong>input/target language combinations were not seen in training</strong>.</li><li>AudioPaLM also demonstrates features of audio language models, such as transferring a voice across languages based on a short spoken prompt.</li></ol><h3 id=key-idea-2>Key idea<a hidden class=anchor aria-hidden=true href=#key-idea-2>#</a></h3><p><strong>A joint vocabulary</strong> that can represent speech and text with a limited number of discrete tokens which, combined with an elementary markup description of tasks, allows training a single decoder-only model on a mixture of tasks that involve arbitrarily interleaved speech and text.</p><p>the underlying architecture of AudioPaLM is a large Transformer model -> can use the weights of a LLM as initial weights.</p><h3 id=related-work>Related work<a hidden class=anchor aria-hidden=true href=#related-work>#</a></h3><p>Ways to fuse text-decoders with advances in non-text encoder models:</p><ul><li>adapter layers: Flamingo, Whisper</li><li>a separate encoder: PaLI</li></ul><p>pro of these two ways: individual components can be frozen while finetuning the model on multimodal data</p><p>con: such models are constrained to only output text, since the decoder is text-only</p><h3 id=model-architecture-2>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture-2>#</a></h3><p>![Screenshot 2025-01-27 at 14.59.13](/Users/aklywtx/Library/Application Support/typora-user-images/Screenshot 2025-01-27 at 14.59.13.png)</p><ol><li><p>how text and audio inputs are tokenized;</p><p>Extract embeddings from the w2v-BERT model and quantize them via k-means. No Normalization before performing the k-means clustering because they found in the multilingual setting that normalization did cause degradation.</p><p>Also tried this on Universal Speech Model (USM) encoder v1, USM-v2(with an auxiliary ASR loss)</p></li><li><p>how we modify existing pretrained text decoders to also model audio;</p><p>In the token embedding matrix $E$, a $t\times m$ matrix, they keep the first t tokens (from zero to t) which correspond to the SentencePiece text tokens while adding the next a tokens (from t to t + a) represent audio tokens. Then train all model parameters.</p></li><li><p>how we convert the model output into raw audio.</p></li></ol><p>‚Äã They experimented with two different methods: i) autoregressive decoding, following the setup of AudioLM; ii) nonautoregressive decoding, using the recently proposed SoundStorm model</p><h3 id=tasks-1>Tasks<a hidden class=anchor aria-hidden=true href=#tasks-1>#</a></h3><ul><li><p>Expressing tasks</p></li><li><p>Combined tasks (combine multiple simple tasks): instruct the model to also output intermediate steps; similar in spirit to chain of thought prompting</p></li></ul><h2 id=paper-4-speechgpt>Paper-4: SpeechGPT<a hidden class=anchor aria-hidden=true href=#paper-4-speechgpt>#</a></h2><p><strong>Titel: SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities</strong></p><h3 id=research-gaps>Research Gaps<a hidden class=anchor aria-hidden=true href=#research-gaps>#</a></h3><ol><li>Images and speech are continuous signals. They can&rsquo;t be adapted directly to LLMs which receive discrete tokens.</li><li>Current speech model: cascading paradigm (the LLM is connected with an automatic speech recognition (ASR) model or a text-tospeech (TTS) model in tandem, or the LLM is employed as a control hub, with several speech processing models) -> limitations: LLM‚Äôs knowledge cannot be transferred to the speech modality; loss of paralinguistic signals; spoken language models only synthesize speech but fail to comprehend its semantic information</li></ol><p><strong>Key idea: speech discretization</strong> speech discretization with a self-supervised trained speech model to unify the modality between speech and text. The discrete speech tokens are then expanded into the vocabulary of the LLM -> model can perceive and generate the speech.</p><p>capacity of follwing multi-modal instructions: build the first speech-text cross-modal instruction-following dataset SpeechInstruct.</p><h3 id=dataset-speechinstruct>Dataset: SpeechInstruct<a hidden class=anchor aria-hidden=true href=#dataset-speechinstruct>#</a></h3><p>a speech-text crossmodal instruction-following dataset</p><ul><li>CrossModal Instruction</li><li>Chain-of-Modality Instruction</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://aklywtx.github.io/tags/multimodal/>Multimodal</a></li></ul><nav class=paginav><a class=next href=https://aklywtx.github.io/posts/nlp_notes-dependency-parser/><span class=title>Next ¬ª</span><br><span>NLP_Notes: Dependency Parser</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Multimodel: Speech-Text Representation Learning on x" href="https://x.com/intent/tweet/?text=Multimodel%3a%20Speech-Text%20Representation%20Learning&amp;url=https%3a%2f%2faklywtx.github.io%2fposts%2fspeech-text-representation-learning%2f&amp;hashtags=Multimodal"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multimodel: Speech-Text Representation Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2faklywtx.github.io%2fposts%2fspeech-text-representation-learning%2f&amp;title=Multimodel%3a%20Speech-Text%20Representation%20Learning&amp;summary=Multimodel%3a%20Speech-Text%20Representation%20Learning&amp;source=https%3a%2f%2faklywtx.github.io%2fposts%2fspeech-text-representation-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multimodel: Speech-Text Representation Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2faklywtx.github.io%2fposts%2fspeech-text-representation-learning%2f&title=Multimodel%3a%20Speech-Text%20Representation%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multimodel: Speech-Text Representation Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2faklywtx.github.io%2fposts%2fspeech-text-representation-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multimodel: Speech-Text Representation Learning on whatsapp" href="https://api.whatsapp.com/send?text=Multimodel%3a%20Speech-Text%20Representation%20Learning%20-%20https%3a%2f%2faklywtx.github.io%2fposts%2fspeech-text-representation-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multimodel: Speech-Text Representation Learning on telegram" href="https://telegram.me/share/url?text=Multimodel%3a%20Speech-Text%20Representation%20Learning&amp;url=https%3a%2f%2faklywtx.github.io%2fposts%2fspeech-text-representation-learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Multimodel: Speech-Text Representation Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Multimodel%3a%20Speech-Text%20Representation%20Learning&u=https%3a%2f%2faklywtx.github.io%2fposts%2fspeech-text-representation-learning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://aklywtx.github.io/>aklywtx</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>