<!doctype html><html lang=en><head><title>NLP_Notes: Dependency Parser | Xin's blog</title>
<meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Background ğŸ”—
dependency grammars:

The syntactic structure of a sentence if described solely in terms of directed binary grammatical relations between the words.

Why dependency grammars?


They can deal with languages that are morphologically rich and have relatively free word order.
The head-dependent relations provide an approximation to the semantic relationship between predicates and their arguments that makes them directly useful for many applications such as coreference resolution, question answering and information extraction.


Dependency Structure: a directed graph G = (V, A)

V: words"><meta name=generator content="Hugo 0.143.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/%20/css/style.css><link rel="shortcut icon" href=/%20/images/favicon.ico type=image/x-icon><script async src="https://www.googletagmanager.com/gtag/js?id=your-google-analytics-id"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","your-google-analytics-id")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css integrity=sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js integrity=sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body><nav class=navigation><a href=/><span class=arrow>â†</span>Home</a>
<a href=/posts>Archive</a>
<a href=/tags>Tags</a>
<a href=/about>About</a></nav><main class=main><section id=single><h1 class=title>NLP_Notes: Dependency Parser</h1><div class=tip><time datetime="2022-12-11 00:00:00 +0000 UTC">Dec 11, 2022</time>
<span class=split>Â·
</span><span>714 words
</span><span class=split>Â·
</span><span>2 minute read</span></div><aside class=toc><details><summary>Table of Contents</summary><div><nav id=TableOfContents><ul><li><a href=#background>Background</a></li><li><a href=#problem-projectivity>Problem: Projectivity</a><ul><li><a href=#solution1-dependency-treebanks>Solution1: Dependency Treebanks</a></li><li><a href=#solution-2-transition-based-dependency-parsing>Solution 2: Transition-Based Dependency Parsing</a></li></ul></li></ul></nav></div></details></aside><div class=content><h2 id=background>Background <a href=#background class=anchor>ğŸ”—</a></h2><ul><li><strong>dependency grammars:</strong></li></ul><p>The syntactic structure of a sentence if described solely in terms of directed binary grammatical relations between the words.</p><ul><li><strong>Why dependency grammars?</strong></li></ul><ol><li>They can deal with languages that are morphologically rich and have relatively free word order.</li><li>The head-dependent relations provide an approximation to the semantic relationship between predicates and their arguments that makes them directly useful for many applications such as coreference resolution, question answering and information extraction.</li></ol><ul><li><strong>Dependency Structure:</strong> a directed graph G = (V, A)</li></ul><p>V: words</p><p>A: relationships between words</p><p>Constraint:</p><ol><li>There is a single designated <strong>root node</strong> that has no incoming arcs. (Why root???)</li><li>With the exception of the root node, each vertex has <strong>exactly one incoming arc</strong>.</li><li>There is <strong>a unique path</strong> from the root node to each vertex in V.</li></ol><br><h2 id=problem-projectivity>Problem: Projectivity <a href=#problem-projectivity class=anchor>ğŸ”—</a></h2><p>What is projective?</p><p>An arc from a head to a dependent is said to be projective if there is a path from the head to every word that lies between the head and the dependent in the sentence. = å¯¹head->dependentä¸­é—´å¤¹ç€çš„æ‰€æœ‰è¯ï¼Œheadéƒ½èƒ½æœ‰è·¯å¾„é€šå‘å®ƒä»¬ã€‚</p><p>An arc from a head to a dependent is said to be projective if there is a path from the head to every word that lies between the head and the dependent in the sentence.</p><blockquote><p>e.g.</p><p>projective: JetBlue canceled our flight this morning.</p><p>non-projective: JetBlue canceled our flight this morning which was already late.</p><p>å¯¹ flight -> was ä¸­é—´çš„ this å’Œ morningï¼Œflightæ²¡æœ‰è·¯å¾„åˆ°å®ƒä»¬ã€‚</p></blockquote><p>A dependency tree is projective if it can be drawn with no crossing edges.</p><br><h3 id=solution1-dependency-treebanks>Solution1: Dependency Treebanks <a href=#solution1-dependency-treebanks class=anchor>ğŸ”—</a></h3><p>adapted from phrase-structure treebanks</p><p>algorithm: ? not understand</p><p><u>shortcoming</u>: inability to easily represent non-projective structures; the lack of internal structure to most noun-phrases(?)</p><br><h3 id=solution-2-transition-based-dependency-parsing>Solution 2: Transition-Based Dependency Parsing <a href=#solution-2-transition-based-dependency-parsing class=anchor>ğŸ”—</a></h3><p>comes from: shift-reduce parsing</p><p>a <strong>stack</strong>: build the parse</p><p>an input <strong>buffer</strong>: list of words</p><p>an <strong>oracle</strong>: predict actions</p><p>initial configuration: stack - [root], buffer - [words]</p><ul><li>If the top 2 elements in the stack have head-dependent relation: left arc or right arc, pop dependent</li><li>else: shift</li></ul><blockquote><p>this is a greedy algorithm</p></blockquote><p><u>shortcoming</u>: greedy: Incorrect choices from oracle will lead to incorrect parses since the parser cannot go back.</p><h4 id=create-an-oracle>Create an oracle <a href=#create-an-oracle class=anchor>ğŸ”—</a></h4><p>To train a classifier, we will need <u>configurations</u> paired with <u>transition operators</u></p><p>Generate <strong>training data</strong>: we derive (configuration, transition) from (sentence, dependency tree):</p><ul><li>Choose LEFTARC if it produces a correct head-dependent relation given the reference parse and the current configuration,</li><li>Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent relation given the reference parse and (2) all of the dependents of the word at the top of the stack have already been assigned,</li><li>Otherwise, choose SHIFT.</li></ul><h4 id=a-feature-based-classifier>A feature-based classifier <a href=#a-feature-based-classifier class=anchor>ğŸ”—</a></h4><p>feature: Word forms, lemmas, parts of speech, the head, the dependency relation to the head</p><p>feature template:</p><p>&lt;s1.w, op>, &lt;S2.w, op> &lt;S2.w, op> &lt;S2.t, op>&lt;b1.w, op>&lt;b1.t, op>&lt;s1.wt, op></p><p>s = stack</p><p>w = word forms</p><p>t = part-of-speech</p><p>op = operator</p><p>Given that the left and right arc transitions operate on the top two elements of the stack, features that combine properties from these positions(namely s1, s2) are even more useful.</p><h4 id=a-neural-classifier>A neural classifier <a href=#a-neural-classifier class=anchor>ğŸ”—</a></h4><h4 id=advanced-methods-in-transition-based-parsing>Advanced methods in transition-based parsing <a href=#advanced-methods-in-transition-based-parsing class=anchor>ğŸ”—</a></h4><p><strong>Alternative transition systems</strong></p><ul><li>LEFTARC: Assert a head-dependent relation between the word at the front of the input buffer and the word at the top of the stack; pop the stack.</li><li>RIGHTARC: Assert a head-dependent relation between the word on the top of the stack and the word at front of the input buffer; shift the word at the front of the input buffer to the stack.</li><li>SHIFT: Remove the word from the front of the input buffer and push it onto the stack.</li><li>REDUCE: Pop the stack.</li></ul><p><strong>Beam search: to explore alternative decision sequences</strong></p></div><div class=tags><a href=https://aklywtx.github.io/tags/nlp-basics>NLP basics</a>
<a href=https://aklywtx.github.io/tags/notes>Notes</a></div><div id=comment><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//your-disqus-shortname.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></section></main><footer id=footer><div class=copyright>Â© Copyright
2025
<span class=split><svg fill="#bbb" width="15" height="15" id="heart-15" width="15" height="15" viewBox="0 0 15 15"><path d="M13.91 6.75c-1.17 2.25-4.3 5.31-6.07 6.94-.1903.1718-.4797.1718-.67.0C5.39 12.06 2.26 9 1.09 6.75-1.48 1.8 5-1.5 7.5 3.45 10-1.5 16.48 1.8 13.91 6.75z"/></svg>
</span>Xin Tong</div><div class=powerby>Powered by <a href=http://www.gohugo.io/>Hugo</a> Theme By <a href=https://github.com/nodejh/hugo-theme-mini>nodejh</a></div></footer></body></html>