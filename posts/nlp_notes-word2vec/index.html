<!doctype html><html lang=en><head><title>NLP_Notes: Word2Vec | Xin's blog</title>
<meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Note on Terminology:
The terms &ldquo;word vectors&rdquo; and &ldquo;word embeddings&rdquo; are often used interchangeably.
The term &ldquo;embedding&rdquo; refers to the fact that we are encoding aspects of a word&rsquo;s meaning in a lower dimensional space.
Main Problem: üîóHow can we represent the meaning of words?
Main Solution: üîóSimilar words will be used in similar contexts.
We can treat words as a vector in a multidimensional semantic space that is derived from  the distribution of word neighbors."><meta name=generator content="Hugo 0.143.0"><meta name=ROBOTS content="NOINDEX, NOFOLLOW"><link rel=stylesheet href=/%20/css/style.css><link rel="shortcut icon" href=/%20/images/favicon.ico type=image/x-icon><script async src="https://www.googletagmanager.com/gtag/js?id=your-google-analytics-id"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","your-google-analytics-id")}</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.css integrity=sha384-RZU/ijkSsFbcmivfdRBQDtwuwVqK7GMOw6IMvKyeWL2K5UAlyp6WonmB8m7Jd0Hn crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/katex.min.js integrity=sha384-pK1WpvzWVBQiP0/GjnvRxV4mOb0oxFuyRxJlk6vVw146n3egcN5C925NCP7a7BY8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.13/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script></head><body><nav class=navigation><a href=/><span class=arrow>‚Üê</span>Home</a>
<a href=/posts>Archive</a>
<a href=/tags>Tags</a>
<a href=/about>About</a></nav><main class=main><section id=single><h1 class=title>NLP_Notes: Word2Vec</h1><div class=tip><time datetime="2022-11-19 00:00:00 +0000 UTC">Nov 19, 2022</time>
<span class=split>¬∑
</span><span>496 words
</span><span class=split>¬∑
</span><span>3 minute read</span></div><aside class=toc><details><summary>Table of Contents</summary><div><nav id=TableOfContents><ul><li><a href=#main-problem>Main Problem:</a></li><li><a href=#main-solution>Main Solution:</a><ul><li><a href=#solution-1-based-on-counts>Solution 1: Based on counts</a></li><li><a href=#solution2-based-on-prediction>Solution2: Based on prediction</a></li></ul></li></ul></nav></div></details></aside><div class=content><p><strong>Note on Terminology:</strong></p><p>The terms &ldquo;word vectors&rdquo; and &ldquo;word embeddings&rdquo; are often used interchangeably.</p><p>The term &ldquo;embedding&rdquo; refers to the fact that we are encoding aspects of a word&rsquo;s meaning in a lower dimensional space.</p><h2 id=main-problem>Main Problem: <a href=#main-problem class=anchor>üîó</a></h2><p>How can we represent the meaning of words?</p><h2 id=main-solution>Main Solution: <a href=#main-solution class=anchor>üîó</a></h2><p>Similar words will be used in similar contexts.</p><p>We can treat words as a vector in a multidimensional semantic space that is derived from the distribution of word neighbors.</p><h3 id=solution-1-based-on-counts>Solution 1: Based on counts <a href=#solution-1-based-on-counts class=anchor>üîó</a></h3><h4 id=co-occurance-matrix>Co-occurance matrix <a href=#co-occurance-matrix class=anchor>üîó</a></h4><ol><li><p>implete the word - word matrix with a window base on count</p><p>e.g.</p><ul><li><p>corpus = &ldquo;I love NLP&rdquo;, &ldquo;I want to eat banana&rdquo;, window = 2 (2 words before and after central word)</p></li><li><p>add &ldquo;START_TOKEN&rdquo; and &ldquo;END_TOKEN&rdquo; -> get &ldquo;START_TOKEN I love NLP END_TOKEN&rdquo;, &ldquo;START_TOKEN I want to eat banana END_TOKEN&rdquo;</p></li><li><table><thead><tr><th><br></th><th>START_TOKEN</th><th>I</th><th>love</th><th>NLP</th><th>&mldr;.</th><th>banana</th></tr></thead><tbody><tr><td>START_TOKEN</td><td>0</td><td>2</td><td>1</td><td></td><td></td><td></td></tr><tr><td>I</td><td>2</td><td>0</td><td>1</td><td></td><td></td><td></td></tr><tr><td>love</td><td>1</td><td></td><td>0</td><td></td><td></td><td></td></tr><tr><td>NLP</td><td>1</td><td></td><td></td><td>0</td><td></td><td></td></tr><tr><td>&mldr;..</td><td></td><td></td><td></td><td></td><td>0</td><td></td></tr><tr><td>banana</td><td></td><td></td><td></td><td></td><td></td><td>0</td></tr></tbody></table></li></ul></li><li><p>Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings</p><p>why? This reduced-dimensionality co-occurrence representation <strong>preserves semantic relationships</strong> between words, e.g. <em>doctor</em> and <em>hospital</em> will be closer than <em>doctor</em> and <em>dog</em> .</p></li></ol><p>The <strong>problem</strong> of of co-occurance matrix: Raw frequency is skewed and not very discriminative: words like <em>the, it, they</em>.</p><p>Solution: <strong>tf-idf weighting</strong>, <strong>PPMI</strong></p><h4 id=tf-idf-weighting--weighted-co-occurance-matrices>tf-idf weighting (weighted co-occurance matrices) <a href=#tf-idf-weighting--weighted-co-occurance-matrices class=anchor>üîó</a></h4><p>Solution: the product of term frequency and document frequency.</p><ol><li><p><strong>tf (# t in d) = count(t, d)</strong></p><p>commonly we squash the raw frequency by log</p></li><li><p><strong>df (# d that t occurs)</strong></p><p>we use the idf (inverse df) because the fewer documents in which a term occurs, the more discriminative the term is and the higher this weight.</p><p><strong>idf = N / df</strong> (N = # documents)</p><p>commonly we squash the raw frequency by log</p></li><li><p>tf-idf weighted value <strong>w = tf x idf</strong></p></li></ol><h4 id=ppmi-pointwise-mutual-information>PPMI (Pointwise Mutual Information) <a href=#ppmi-pointwise-mutual-information class=anchor>üîó</a></h4><p>for term-term-matrices</p><p>intuition: how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.</p><p>How often two events x and y occur, compared to that they occur independetly:</p><p>$$
I(x, y) = log_{2} \frac{P(x, y)}{P(x)P(y)}
$$
we substitute x, y with a target word $w$ and a context word $$c$$:
$$
PMI(w, c) = log_{2} \frac{P(w, c)}{P(w)P(c)}
$$</p><p>(We assume the independence of w and c, so the denominater is just the product of the unigram probability. )</p><p>Because the negative PMI tend to be unreliable, we replace all negative PMI values zero (That&rsquo;s how positive comes)</p><p>$$
PPMI(w, c) = max(log_{2} \frac{P(w, c)}{P(w)P(c)}, 0)
$$</p><p>problem: very rare words tend to have very high PMI values</p><p>solution: slightly change the computation for P(c), raise the probability of the context word to the power of \alpha
$$
PPMI(w, c) = max(log_{2} \frac{P(w, c)}{P(w)P_\alpha(c)}, 0)
$$</p><h3 id=solution2-based-on-prediction>Solution2: Based on prediction <a href=#solution2-based-on-prediction class=anchor>üîó</a></h3><h4 id=word2vec>word2vec <a href=#word2vec class=anchor>üîó</a></h4><p>why two vectors per word?(a context vector, a central vector, average both at end)</p><p>Easy optimization. One vector makes the computation much more complex.</p></div><div class=tags><a href=https://aklywtx.github.io/tags/nlp-basics>NLP basics</a>
<a href=https://aklywtx.github.io/tags/notes>Notes</a></div><div id=comment><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//your-disqus-shortname.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div></section></main><footer id=footer><div class=copyright>¬© Copyright
2025
<span class=split><svg fill="#bbb" width="15" height="15" id="heart-15" width="15" height="15" viewBox="0 0 15 15"><path d="M13.91 6.75c-1.17 2.25-4.3 5.31-6.07 6.94-.1903.1718-.4797.1718-.67.0C5.39 12.06 2.26 9 1.09 6.75-1.48 1.8 5-1.5 7.5 3.45 10-1.5 16.48 1.8 13.91 6.75z"/></svg>
</span>Xin Tong</div><div class=powerby>Powered by <a href=http://www.gohugo.io/>Hugo</a> Theme By <a href=https://github.com/nodejh/hugo-theme-mini>nodejh</a></div></footer></body></html>