<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NLP Basics on Xin's blog</title><link>https://aklywtx.github.io/tags/nlp-basics/</link><description>Recent content in NLP Basics on Xin's blog</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sun, 11 Dec 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://aklywtx.github.io/tags/nlp-basics/index.xml" rel="self" type="application/rss+xml"/><item><title>NLP_Notes: Dependency Parser</title><link>https://aklywtx.github.io/posts/nlp_notes-dependency-parser/</link><pubDate>Sun, 11 Dec 2022 00:00:00 +0000</pubDate><guid>https://aklywtx.github.io/posts/nlp_notes-dependency-parser/</guid><description>&lt;h2 id="background">Background &lt;a href="#background" class="anchor">ðŸ”—&lt;/a>&lt;/h2>&lt;ul>
&lt;li>&lt;strong>dependency grammars:&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>The syntactic structure of a sentence if described solely in terms of directed binary grammatical relations between the words.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Why dependency grammars?&lt;/strong>&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>They can deal with languages that are morphologically rich and have relatively free word order.&lt;/li>
&lt;li>The head-dependent relations provide an approximation to the semantic relationship between predicates and their arguments that makes them directly useful for many applications such as coreference resolution, question answering and information extraction.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;strong>Dependency Structure:&lt;/strong> a directed graph G = (V, A)&lt;/li>
&lt;/ul>
&lt;p>V: words&lt;/p></description></item><item><title>NLP_Notes: Word2Vec</title><link>https://aklywtx.github.io/posts/nlp_notes-word2vec/</link><pubDate>Sat, 19 Nov 2022 00:00:00 +0000</pubDate><guid>https://aklywtx.github.io/posts/nlp_notes-word2vec/</guid><description>&lt;p>&lt;strong>Note on Terminology:&lt;/strong>&lt;/p>
&lt;p>The terms &amp;ldquo;word vectors&amp;rdquo; and &amp;ldquo;word embeddings&amp;rdquo; are often used interchangeably.&lt;/p>
&lt;p>The term &amp;ldquo;embedding&amp;rdquo; refers to the fact that we are encoding aspects of a word&amp;rsquo;s meaning in a lower dimensional space.&lt;/p>
&lt;h2 id="main-problem">Main Problem: &lt;a href="#main-problem" class="anchor">ðŸ”—&lt;/a>&lt;/h2>&lt;p>How can we represent the meaning of words?&lt;/p>
&lt;h2 id="main-solution">Main Solution: &lt;a href="#main-solution" class="anchor">ðŸ”—&lt;/a>&lt;/h2>&lt;p>Similar words will be used in similar contexts.&lt;/p>
&lt;p>We can treat words as a vector in a multidimensional semantic space that is derived from the distribution of word neighbors.&lt;/p></description></item></channel></rss>